# -*- coding: utf-8 -*-
"""gpt-oss-(20B)-Fine-tuning.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/gpt-oss-(20B)-Fine-tuning.ipynb

To run this, press "*Runtime*" and press "*Run all*" on a **free** Tesla T4 Google Colab instance!
<div class="align-center">
<a href="https://unsloth.ai/"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
<a href="https://discord.gg/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord button.png" width="145"></a>
<a href="https://docs.unsloth.ai/"><img src="https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true" width="125"></a></a> Join Discord if you need help + ⭐ <i>Star us on <a href="https://github.com/unslothai/unsloth">Github</a> </i> ⭐
</div>

To install Unsloth on your own computer, follow the installation instructions on our Github page [here](https://docs.unsloth.ai/get-started/installing-+-updating).

You will learn how to do [data prep](#Data), how to [train](#Train), how to [run the model](#Inference), & [how to save it](#Save)

### News

**NEW!** Unsloth now supports OpenAI's **gpt-oss**! We fixed issues in the model and **[Read our Guide](https://docs.unsloth.ai/basics/gpt-oss)** for more info!

Unsloth now supports Text-to-Speech (TTS) models. Read our [guide here](https://docs.unsloth.ai/basics/text-to-speech-tts-fine-tuning).

Read our **[Gemma 3N Guide](https://docs.unsloth.ai/basics/gemma-3n-how-to-run-and-fine-tune)** and check out our new **[Dynamic 2.0](https://docs.unsloth.ai/basics/unsloth-dynamic-2.0-ggufs)** quants which outperforms other quantization methods!

Visit our docs for all our [model uploads](https://docs.unsloth.ai/get-started/all-our-models) and [notebooks](https://docs.unsloth.ai/get-started/unsloth-notebooks).

### Installation
"""

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# # We're installing the latest Torch, Triton, OpenAI's Triton kernels, Transformers and Unsloth!
# !pip install --upgrade -qqq uv
# try: import numpy; install_numpy = f"numpy=={numpy.__version__}"
# except: install_numpy = "numpy"
# !uv pip install -qqq \
#     "torch>=2.8.0" "triton>=3.4.0" {install_numpy} \
#     "unsloth_zoo[base] @ git+https://github.com/unslothai/unsloth-zoo" \
#     "unsloth[base] @ git+https://github.com/unslothai/unsloth" \
#     torchvision bitsandbytes \
#     git+https://github.com/huggingface/transformers \
#     git+https://github.com/triton-lang/triton.git@main#subdirectory=python/triton_kernels
#

"""### Unsloth

We're about to demonstrate the power of the new OpenAI GPT-OSS 20B model through an inference example. For our `MXFP4` version, use this [notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/GPT_OSS_MXFP4_(20B)-Inference.ipynb) instead.
"""

import os
# Ensure we don't try to use MXFP4 dequantization which can OOM and trigger downloads
os.environ.setdefault("TRANSFORMERS_NO_MXFP4", "1")
os.environ.setdefault("HF_HUB_OFFLINE", "1")
os.environ.setdefault("HF_HUB_DISABLE_TELEMETRY", "1")
# Ensure Unsloth doesn't enable torch.compile (causes Dynamo issues on this setup)
os.environ.setdefault("UNSLOTH_DISABLE_TORCH_COMPILE", "1")
# Allow training with models loaded via device_map without Accelerate blocking
os.environ.setdefault("ACCELERATE_BYPASS_DEVICE_MAP", "true")
os.environ.setdefault("TOKENIZERS_PARALLELISM", "false")
SMOKE_MODE = os.environ.get("SMOKE_MODE", "0") == "1"
if SMOKE_MODE:
    # Prefer float16 compute for a lighter smoke path
    os.environ.setdefault("UNSLOTH_MIXED_PRECISION", "float16")
import unsloth  # Must be imported before transformers so Unsloth patches apply
import sys
import time
import torch
"""
Workaround: Transformers 4.55 may try to initialize weights for
GptOssTopKRouter, but some accelerated router implementations do not
expose .weight/.bias as parameters. We skip router param init.
"""
try:
    from transformers.models.gpt_oss import modeling_gpt_oss as _gptoss_mod
    _orig_init_weights = _gptoss_mod.GptOssPreTrainedModel._init_weights
    def _patched_init_weights(self, module):
        if isinstance(module, (_gptoss_mod.GptOssTopKRouter, _gptoss_mod.GptOssExperts)):
            return
        return _orig_init_weights(self, module)
    _gptoss_mod.GptOssPreTrainedModel._init_weights = _patched_init_weights
except Exception:
    pass

# Disable Unsloth's strict uninitialized-weights handler to allow local GPT-OSS load
try:
    from transformers.modeling_utils import logger as _tlogger
    import unsloth.models._utils as _uutils
    # Hard-disable raise-on-uninitialized
    if hasattr(_uutils, "_RaiseUninitialized"):
        try:
            _uutils._RaiseUninitialized.emit = lambda self, record: None
        except Exception:
            pass
    for _h in list(_tlogger.handlers):
        if _h.__class__.__name__ == "_RaiseUninitialized":
            _tlogger.removeHandler(_h)
except Exception:
    pass

# Workaround: Unsloth's SFTTrainer calls fix_untrained_tokens which inspects lm_head weights.
# On some meta/offload setups this raises NotImplementedError (copy out of meta tensor).
# We disable it for this local sanity run.
try:
    import unsloth_zoo.tokenizer_utils as _tz
    _tz.fix_untrained_tokens = lambda *args, **kwargs: None
except Exception:
    pass

max_seq_length = 4096
# Allow overriding max seq length via env, and shrink in smoke mode
_env_msl = os.environ.get("MAX_SEQ_LENGTH")
if _env_msl and _env_msl.isdigit():
    max_seq_length = int(_env_msl)
elif SMOKE_MODE:
    max_seq_length = 1024
dtype = None

# 4bit pre quantized models we support for 4x faster downloading + no OOMs.
fourbit_models = [
    "unsloth/gpt-oss-20b-unsloth-bnb-4bit", # 20B model using bitsandbytes 4bit quantization
    "unsloth/gpt-oss-120b-unsloth-bnb-4bit",
    "unsloth/gpt-oss-20b", # 20B model using MXFP4 format
    "unsloth/gpt-oss-120b",
] # More models at https://huggingface.co/unsloth

LOCAL_MODEL_DIR = "/home/tdeshane/gpt-oss/gpt-oss-20b-final"
MODEL_NAME = os.environ.get("MODEL_DIR", LOCAL_MODEL_DIR if os.path.isdir(LOCAL_MODEL_DIR) else "unsloth/gpt-oss-20b")

# Mode selection: manual|smoke|full
MODE = os.environ.get("MODE", "").lower()
if MODE == "manual":
    os.environ["MANUAL_STEP"] = "1"
elif MODE == "smoke":
    os.environ["SMOKE_MODE"] = "1"
    os.environ.setdefault("MAX_STEPS", "1")

USE_UNSLOTH = os.environ.get("ENABLE_UNSLOTH", "0") == "1"
USING_UNSLOTH = False

_t0 = time.time()

if USE_UNSLOTH:
    try:
        from unsloth import FastLanguageModel
        model, tokenizer = FastLanguageModel.from_pretrained(
            model_name = MODEL_NAME,
            dtype = dtype, # None for auto detection
            max_seq_length = max_seq_length, # Choose any for long context!
            load_in_4bit = True,  # 4 bit quantization to reduce memory
            full_finetuning = False, # [NEW!] We have full finetuning now!
            local_files_only = True, # Prefer local weights; avoid downloading
            device_map = {"": 0},  # force single-GPU, no CPU offload
            low_cpu_mem_usage = True,
            offload_state_dict = False,
            trust_remote_code = True,
            max_memory = {0: "20GiB"},
            # token = "hf_...", # use one if using gated models
        )
        USING_UNSLOTH = True
    except Exception as e:
        print(f"Unsloth load failed ({e}). Falling back to Transformers + PEFT.")

if not USING_UNSLOTH:
    from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
    from peft import LoraConfig, get_peft_model

    tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, local_files_only=True, use_fast=True)
    bnb_config = BitsAndBytesConfig(
        load_in_4bit=True,
        bnb_4bit_use_double_quant=True,
        bnb_4bit_quant_type="nf4",
        bnb_4bit_compute_dtype=torch.float16 if SMOKE_MODE else torch.bfloat16,
    )
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_NAME,
        trust_remote_code=True,
        local_files_only=True,
        low_cpu_mem_usage=True,
        quantization_config=bnb_config,
        device_map={"": 0},  # single-GPU, no CPU offload
        offload_state_dict=False,
        max_memory={0: "20GiB"},
    )

print(f"Model+tokenizer load took {time.time() - _t0:.2f}s")

# Prefer eager attention for smoke to avoid kernel init overhead
try:
    if SMOKE_MODE or os.environ.get("ATTN_IMPL", "").lower() == "eager":
        if hasattr(model, "config"):
            model.config._attn_implementation = "eager"
        os.environ["XFORMERS_DISABLED"] = "1"
except Exception:
    pass

"""We now add LoRA adapters for parameter efficient finetuning - this allows us to only efficiently train 1% of all parameters."""

if USING_UNSLOTH:
    _t_peft0 = time.time()
    model = FastLanguageModel.get_peft_model(
        model,
        r = 8, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128
        target_modules = ["q_proj", "k_proj", "v_proj", "o_proj",
                         "gate_proj", "up_proj", "down_proj",],
        lora_alpha = 16,
        lora_dropout = 0, # Supports any, but = 0 is optimized
        bias = "none",    # Supports any, but = "none" is optimized
        # [NEW] "unsloth" uses 30% less VRAM, fits 2x larger batch sizes!
        use_gradient_checkpointing = "unsloth", # True or "unsloth" for very long context
        random_state = 3407,
        use_rslora = False,  # We support rank stabilized LoRA
        loftq_config = None, # And LoftQ
    )
    print(f"LoRA PEFT wrap took {time.time() - _t_peft0:.2f}s")
else:
    from peft import LoraConfig, get_peft_model
    _t_peft0 = time.time()
    lora_config = LoraConfig(
        r=8,
        lora_alpha=16,
        lora_dropout=0.0,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
        bias="none",
        task_type="CAUSAL_LM",
    )
    model = get_peft_model(model, lora_config)
    print(f"LoRA PEFT wrap took {time.time() - _t_peft0:.2f}s")

"""### Reasoning Effort
The `gpt-oss` models from OpenAI include a feature that allows users to adjust the model's "reasoning effort." This gives you control over the trade-off between the model's performance and its response speed (latency) which by the amount of token the model will use to think.

----

The `gpt-oss` models offer three distinct levels of reasoning effort you can choose from:

* **Low**: Optimized for tasks that need very fast responses and don't require complex, multi-step reasoning.
* **Medium**: A balance between performance and speed.
* **High**: Provides the strongest reasoning performance for tasks that require it, though this results in higher latency.
"""

from transformers import TextStreamer

if os.getenv("RUN_DEMO", "0") == "1":
    messages = [
        {"role": "user", "content": "Solve x^5 + 3x^4 - 10 = 3."},
    ]
    try:
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt = True,
            return_tensors = "pt",
            return_dict = True,
            reasoning_effort = "low",
        )
    except Exception:
        # Fallback if no chat_template
        prompt = messages[0]["content"] if isinstance(messages[0], dict) else str(messages)
        inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    _ = model.generate(**inputs, max_new_tokens = 512, streamer = TextStreamer(tokenizer))

"""Changing the `reasoning_effort` to `medium` will make the model think longer. We have to increase the `max_new_tokens` to occupy the amount of the generated tokens but it will give better and more correct answer"""

from transformers import TextStreamer

if os.getenv("RUN_DEMO", "0") == "1":
    messages = [
        {"role": "user", "content": "Solve x^5 + 3x^4 - 10 = 3."},
    ]
    try:
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt = True,
            return_tensors = "pt",
            return_dict = True,
            reasoning_effort = "medium",
        )
    except Exception:
        prompt = messages[0]["content"] if isinstance(messages[0], dict) else str(messages)
        inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    _ = model.generate(**inputs, max_new_tokens = 1024, streamer = TextStreamer(tokenizer))

"""Lastly we will test it using `reasoning_effort` to `high`"""

from transformers import TextStreamer

if os.getenv("RUN_DEMO", "0") == "1":
    messages = [
        {"role": "user", "content": "Solve x^5 + 3x^4 - 10 = 3."},
    ]
    try:
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt = True,
            return_tensors = "pt",
            return_dict = True,
            reasoning_effort = "high",
        )
    except Exception:
        prompt = messages[0]["content"] if isinstance(messages[0], dict) else str(messages)
        inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}

    _ = model.generate(**inputs, max_new_tokens = 2048, streamer = TextStreamer(tokenizer))

"""<a name="Data"></a>
### Data Prep

The `HuggingFaceH4/Multilingual-Thinking` dataset will be utilized as our example. This dataset, available on Hugging Face, contains reasoning chain-of-thought examples derived from user questions that have been translated from English into four other languages. It is also the same dataset referenced in OpenAI's [cookbook](https://cookbook.openai.com/articles/gpt-oss/fine-tune-transfomers) for fine-tuning. The purpose of using this dataset is to enable the model to learn and develop reasoning capabilities in these four distinct languages.
"""

def _render_messages(convo):
    try:
        return tokenizer.apply_chat_template(
            convo,
            tokenize = False,
            add_generation_prompt = False,
        )
    except Exception:
        # Fallback: naive role: content formatting
        parts = []
        for msg in convo:
            role = msg.get("role", "user") if isinstance(msg, dict) else "user"
            content = msg.get("content", "") if isinstance(msg, dict) else str(msg)
            parts.append(f"{role}: {content}")
        return "\n".join(parts)

def formatting_prompts_func(examples):
    convos = examples["messages"]
    texts = [_render_messages(convo) for convo in convos]
    return { "text" : texts, }
pass

from datasets import load_dataset, Dataset

try:
    dataset = load_dataset("HuggingFaceH4/Multilingual-Thinking", split="train")
except Exception:
    # Offline fallback: create a tiny synthetic dataset
    synthetic_messages = [
        [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Solve x^2 - 4x + 4 = 0."},
            {"role": "assistant", "content": "The solution is x=2."},
        ],
        [
            {"role": "system", "content": "You are a helpful assistant."},
            {"role": "user", "content": "Translate 'hello' to French."},
            {"role": "assistant", "content": "bonjour"},
        ],
    ]
    dataset = Dataset.from_dict({"messages": synthetic_messages})

"""To format our dataset, we will apply our version of the GPT OSS prompt"""

_t_data0 = time.time()
try:
    from unsloth.chat_templates import standardize_sharegpt
    dataset = standardize_sharegpt(dataset)
except Exception:
    # Fallback: use dataset as-is if Unsloth utilities unavailable
    pass
dataset = dataset.map(formatting_prompts_func, batched = True,)
print(f"Dataset standardize+format took {time.time() - _t_data0:.2f}s")

"""Let's take a look at the dataset, and check what the 1st example shows"""

print(dataset[0]['text'])

# Optional ultra-minimal manual train step to avoid Trainer/Accelerate overhead
if os.getenv("MANUAL_STEP", "0") == "1":
    try:
        model.train()
        # Build a single micro-batch
        num_samples = min(1, len(dataset))
        texts = dataset.select(range(num_samples))["text"]
        enc = tokenizer(
            texts,
            return_tensors="pt",
            padding="longest",
            truncation=True,
            max_length=max_seq_length,
        )
        input_ids = enc["input_ids"].to(model.device)
        attention_mask = enc.get("attention_mask")
        if attention_mask is not None:
            attention_mask = attention_mask.to(model.device)
        labels = input_ids.clone()

        # Optimizer over only trainable (LoRA) parameters
        trainable_params = [p for p in model.parameters() if getattr(p, "requires_grad", False)]
        if len(trainable_params) == 0:
            raise RuntimeError("No trainable parameters found for manual step.")
        optimizer = torch.optim.AdamW(trainable_params, lr=2e-4)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
        loss = outputs.loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad(set_to_none=True)
        print(f"Manual step loss: {loss.item():.6f}")
        if os.getenv("SAVE_ADAPTERS", "0") == "1":
            out_dir = os.environ.get("ADAPTER_OUTPUT_DIR", "outputs/lora_adapters")
            os.makedirs(out_dir, exist_ok=True)
            model.save_pretrained(out_dir)
            print(f"Saved LoRA adapters to: {out_dir}")
    except Exception as e:
        print(f"Manual training step failed: {e}")
        sys.exit(1)
    # Exit after manual smoke step
    sys.exit(0)

"""What is unique about GPT-OSS is that it uses OpenAI [Harmony](https://github.com/openai/harmony) format which support conversation structures, reasoning output, and tool calling.

<a name="Train"></a>
### Train the model
Now let's use Huggingface TRL's `SFTTrainer`! More docs here: [TRL SFT docs](https://huggingface.co/docs/trl/sft_trainer). We do 60 steps to speed things up, but you can set `num_train_epochs=1` for a full run, and turn off `max_steps=None`.
"""

from trl import SFTConfig, SFTTrainer
# Safety: avoid HF Trainer moving meta/offloaded tensors with .to()
try:
    from transformers.trainer import Trainer as _HFTrainer
    if hasattr(_HFTrainer, "_move_model_to_device"):
        def _noop_move(self, model, device):
            return
        _HFTrainer._move_model_to_device = _noop_move
except Exception:
    pass
from transformers import DataCollatorForSeq2Seq

# Allow overriding max_steps via env var for quick sanity runs
_env_max_steps = os.environ.get("MAX_STEPS")
if _env_max_steps and _env_max_steps.strip().lstrip("-").isdigit():
    _max_steps = int(_env_max_steps)
else:
    _max_steps = 60 if USING_UNSLOTH else 20

# Build training args first so we can set placement before trainer init
sft_args = SFTConfig(
    per_device_train_batch_size = 1,
    gradient_accumulation_steps = 1,
    warmup_steps = 5,
    # num_train_epochs = 1, # Set this for 1 full training run.
    max_steps = _max_steps,
    do_train = True,
    fp16_full_eval = False,
    bf16_full_eval = False,
    bf16 = False if SMOKE_MODE else True,
    fp16 = True if SMOKE_MODE else False,
    gradient_checkpointing = True,
    gradient_checkpointing_kwargs = {"use_reentrant": False},
    dataset_num_proc = 1,
    learning_rate = 2e-4,
    logging_steps = 1,
    optim = "adamw_8bit",
    weight_decay = 0.01,
    lr_scheduler_type = "linear",
    seed = 3407,
    output_dir = "outputs",
    report_to = "none", # Use this for WandB etc
)

# Prevent Trainer from calling model.to(device) on meta/offloaded tensors
try:
    setattr(sft_args, "place_model_on_device", False)
except Exception:
    pass

# Additionally hint to HF Trainer that the model is BnB quantized (skip .to())
try:
    from transformers.utils.quantization_config import QuantizationMethod
    setattr(model, "quantization_method", QuantizationMethod.BITS_AND_BYTES)
except Exception:
    pass

_t_trn0 = time.time()
trainer = SFTTrainer(
    model = model,
    tokenizer = tokenizer,
    train_dataset = dataset,
    args = sft_args,
    optimizers=(None, None),
)
print(f"Trainer init took {time.time() - _t_trn0:.2f}s")

# @title Show current memory stats
# Ensure trainer does not try to move model (avoids meta -> device copy)
try:
    if hasattr(trainer, "args"):
        setattr(trainer.args, "place_model_on_device", False)
    # Some Trainer versions cache this on the instance
    if hasattr(trainer, "place_model_on_device"):
        trainer.place_model_on_device = False
except Exception:
    pass

gpu_stats = torch.cuda.get_device_properties(0)
start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)
print(f"GPU = {gpu_stats.name}. Max memory = {max_memory} GB.")
print(f"{start_gpu_memory} GB of memory reserved.")

_t_train0 = time.time()
trainer_stats = trainer.train()
print(f"Trainer train() took {time.time() - _t_train0:.2f}s")

if os.getenv("SAVE_ADAPTERS", "0") == "1":
    out_dir = os.environ.get("ADAPTER_OUTPUT_DIR", "outputs/lora_adapters")
    os.makedirs(out_dir, exist_ok=True)
    model.save_pretrained(out_dir)
    print(f"Saved LoRA adapters to: {out_dir}")

# @title Show final memory and time stats
used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)
used_memory_for_lora = round(used_memory - start_gpu_memory, 3)
used_percentage = round(used_memory / max_memory * 100, 3)
lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)
print(f"{trainer_stats.metrics['train_runtime']} seconds used for training.")
print(
    f"{round(trainer_stats.metrics['train_runtime']/60, 2)} minutes used for training."
)
print(f"Peak reserved memory = {used_memory} GB.")
print(f"Peak reserved memory for training = {used_memory_for_lora} GB.")
print(f"Peak reserved memory % of max memory = {used_percentage} %.")
print(f"Peak reserved memory for training % of max memory = {lora_percentage} %.")

"""<a name="Inference"></a>
### Inference
Let's run the model! You can change the instruction and input - leave the output blank!
"""

if os.getenv("RUN_DEMO", "0") == "1":
    messages = [
        {"role": "system", "content": "reasoning language: French\n\nYou are a helpful assistant that can solve mathematical problems."},
        {"role": "user", "content": "Solve x^5 + 3x^4 - 10 = 3."},
    ]
    try:
        inputs = tokenizer.apply_chat_template(
            messages,
            add_generation_prompt = True,
            return_tensors = "pt",
            return_dict = True,
            reasoning_effort = "medium",
        )
    except Exception:
        # Compose a simple prompt string
        prompt = messages[0]["content"] + "\n\n" + messages[1]["content"]
        inputs = tokenizer(prompt, return_tensors="pt")
    inputs = {k: v.to(model.device) for k, v in inputs.items()}
    from transformers import TextStreamer
    _ = model.generate(**inputs, max_new_tokens = 2048, streamer = TextStreamer(tokenizer))

"""And we're done! If you have any questions on Unsloth, we have a [Discord](https://discord.gg/unsloth) channel! If you find any bugs or want to keep updated with the latest LLM stuff, or need help, join projects etc, feel free to join our Discord!

Some other links:
1. Train your own reasoning model - Llama GRPO notebook [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.1_(8B)-GRPO.ipynb)
2. Saving finetunes to Ollama. [Free notebook](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3_(8B)-Ollama.ipynb)
3. Llama 3.2 Vision finetuning - Radiography use case. [Free Colab](https://colab.research.google.com/github/unslothai/notebooks/blob/main/nb/Llama3.2_(11B)-Vision.ipynb)
6. See notebooks for DPO, ORPO, Continued pretraining, conversational finetuning and more on our [documentation](https://docs.unsloth.ai/get-started/unsloth-notebooks)!

<div class="align-center">
  <a href="https://unsloth.ai"><img src="https://github.com/unslothai/unsloth/raw/main/images/unsloth%20new%20logo.png" width="115"></a>
  <a href="https://discord.gg/unsloth"><img src="https://github.com/unslothai/unsloth/raw/main/images/Discord.png" width="145"></a>
  <a href="https://docs.unsloth.ai/"><img src="https://github.com/unslothai/unsloth/blob/main/images/documentation%20green%20button.png?raw=true" width="125"></a>

  Join Discord if you need help + ⭐️ <i>Star us on <a href="https://github.com/unslothai/unsloth">Github</a> </i> ⭐️
</div>

"""